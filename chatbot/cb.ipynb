{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding,Dropout, Bidirectional, Input, merge, Flatten, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import json\n",
    "from data_utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "def load_cornell_movie_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, start_line, limit)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_twitter_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_sentences(\"ShortenTwitterAsciiCorpus.txt\")[1000:2000]\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_friends_data():\n",
    "    sentences = read_lines(\"friends-final.txt\", \"\t\", 2, 5, start_line = 0, limit = 10000)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\Project\\MLPractice\\chatbot\\model.py:36: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  output_att = merge([inputs, att_prob ], mode='mul')\n",
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 200)      320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4000)         0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 20, 4000)     0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 200)      800200      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 20, 200)      0           bidirectional_1[0][0]            \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 200)      320800      merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 20, 200)      320800      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 20, 200)      320800      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 10000)    2010000     lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 10000)    0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 7,093,400\n",
      "Trainable params: 7,093,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 100, 3)\n",
    "model = create_model_with_attention(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 300, 100, 3)\n",
    "print(model.summary())\n",
    "#model.load_weights(\"0new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict, idx_dict = load_vocab(\"vocab_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data: 8000 1000\n",
      "(435, 20, 10000)\n",
      "            i've loved you ever since i can remember long before you married tom dickson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "435/435 [==============================] - 11s 26ms/step - loss: 8.7144 - acc: 0.4125\n",
      "Epoch 2/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 4.8276 - acc: 0.5448\n",
      "Epoch 3/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.7265 - acc: 0.5448\n",
      "Epoch 4/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.4993 - acc: 0.5448\n",
      "Epoch 5/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.4411 - acc: 0.5448\n",
      "Epoch 6/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.4131 - acc: 0.5448\n",
      "Epoch 7/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.4010 - acc: 0.5448\n",
      "Epoch 8/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3890 - acc: 0.5448\n",
      "Epoch 9/100\n",
      "435/435 [==============================] - 10s 22ms/step - loss: 3.3765 - acc: 0.5448\n",
      "Epoch 10/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3664 - acc: 0.5448\n",
      "Epoch 11/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3590 - acc: 0.5448\n",
      "Epoch 12/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3557 - acc: 0.5448\n",
      "Epoch 13/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3449 - acc: 0.5448\n",
      "Epoch 14/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3517 - acc: 0.5448\n",
      "Epoch 15/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3425 - acc: 0.5448\n",
      "Epoch 16/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3269 - acc: 0.5448\n",
      "Epoch 17/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3402 - acc: 0.5448\n",
      "Epoch 18/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3268 - acc: 0.5448\n",
      "Epoch 19/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3124 - acc: 0.5448\n",
      "Epoch 20/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3240 - acc: 0.5448\n",
      "Epoch 21/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.2989 - acc: 0.5448\n",
      "Epoch 22/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.3004 - acc: 0.5448\n",
      "Epoch 23/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 3.2852 - acc: 0.5448\n",
      "Epoch 24/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.2602 - acc: 0.5448\n",
      "Epoch 25/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.2504 - acc: 0.5448\n",
      "Epoch 26/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.2237 - acc: 0.5448\n",
      "Epoch 27/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.1851 - acc: 0.5449\n",
      "Epoch 28/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.1791 - acc: 0.5448\n",
      "Epoch 29/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.0559 - acc: 0.5448\n",
      "Epoch 30/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 3.0104 - acc: 0.5448\n",
      "Epoch 31/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 3.0086 - acc: 0.5448\n",
      "Epoch 32/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9768 - acc: 0.5448\n",
      "Epoch 33/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9342 - acc: 0.5448\n",
      "Epoch 34/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9633 - acc: 0.5448\n",
      "Epoch 35/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9023 - acc: 0.5448\n",
      "Epoch 36/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9285 - acc: 0.5448\n",
      "Epoch 37/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.9002 - acc: 0.5448\n",
      "Epoch 38/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8442 - acc: 0.5451\n",
      "Epoch 39/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8358 - acc: 0.5501\n",
      "Epoch 40/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8896 - acc: 0.5515\n",
      "Epoch 41/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8362 - acc: 0.5503\n",
      "Epoch 42/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8283 - acc: 0.5499\n",
      "Epoch 43/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 2.8249 - acc: 0.5484\n",
      "Epoch 44/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8362 - acc: 0.5508\n",
      "Epoch 45/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 2.7922 - acc: 0.5532\n",
      "Epoch 46/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8187 - acc: 0.5508\n",
      "Epoch 47/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8178 - acc: 0.5508\n",
      "Epoch 48/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7920 - acc: 0.5499\n",
      "Epoch 49/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7519 - acc: 0.5517\n",
      "Epoch 50/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8565 - acc: 0.5462\n",
      "Epoch 51/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7531 - acc: 0.5524\n",
      "Epoch 52/100\n",
      "435/435 [==============================] - 10s 22ms/step - loss: 2.7342 - acc: 0.5508\n",
      "Epoch 53/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.8158 - acc: 0.5493\n",
      "Epoch 54/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7368 - acc: 0.5524\n",
      "Epoch 55/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7287 - acc: 0.5513\n",
      "Epoch 56/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7452 - acc: 0.5515\n",
      "Epoch 57/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7189 - acc: 0.5522\n",
      "Epoch 58/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7442 - acc: 0.5526\n",
      "Epoch 59/100\n",
      "435/435 [==============================] - 10s 22ms/step - loss: 2.7399 - acc: 0.5516\n",
      "Epoch 60/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6968 - acc: 0.5522\n",
      "Epoch 61/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7152 - acc: 0.5523\n",
      "Epoch 62/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6796 - acc: 0.5533\n",
      "Epoch 63/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 2.7219 - acc: 0.5525\n",
      "Epoch 64/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6597 - acc: 0.5544\n",
      "Epoch 65/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.7039 - acc: 0.5518\n",
      "Epoch 66/100\n",
      "435/435 [==============================] - 10s 22ms/step - loss: 2.6645 - acc: 0.5528\n",
      "Epoch 67/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6253 - acc: 0.5551\n",
      "Epoch 68/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6861 - acc: 0.5511\n",
      "Epoch 69/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6459 - acc: 0.5541\n",
      "Epoch 70/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6486 - acc: 0.5547\n",
      "Epoch 71/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6307 - acc: 0.5553\n",
      "Epoch 72/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6015 - acc: 0.5559\n",
      "Epoch 73/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6744 - acc: 0.5497\n",
      "Epoch 74/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6017 - acc: 0.5557\n",
      "Epoch 75/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6194 - acc: 0.5533\n",
      "Epoch 76/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6021 - acc: 0.5538\n",
      "Epoch 77/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6410 - acc: 0.5506\n",
      "Epoch 78/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5870 - acc: 0.5557\n",
      "Epoch 79/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6011 - acc: 0.5560\n",
      "Epoch 80/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5762 - acc: 0.5556\n",
      "Epoch 81/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5442 - acc: 0.5574\n",
      "Epoch 82/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.6212 - acc: 0.5553\n",
      "Epoch 83/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5849 - acc: 0.5544\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5830 - acc: 0.5544\n",
      "Epoch 85/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5546 - acc: 0.5568\n",
      "Epoch 86/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5275 - acc: 0.5567\n",
      "Epoch 87/100\n",
      "435/435 [==============================] - 9s 22ms/step - loss: 2.6005 - acc: 0.5541\n",
      "Epoch 88/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5547 - acc: 0.5566\n",
      "Epoch 89/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5380 - acc: 0.5547\n",
      "Epoch 90/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5188 - acc: 0.5553\n",
      "Epoch 91/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5880 - acc: 0.5546\n",
      "Epoch 92/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5265 - acc: 0.5567\n",
      "Epoch 93/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5074 - acc: 0.5569\n",
      "Epoch 94/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5512 - acc: 0.5551\n",
      "Epoch 95/100\n",
      "435/435 [==============================] - 10s 22ms/step - loss: 2.5050 - acc: 0.5577\n",
      "Epoch 96/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.4870 - acc: 0.5571\n",
      "Epoch 97/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5797 - acc: 0.5536\n",
      "Epoch 98/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5050 - acc: 0.5576\n",
      "Epoch 99/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.5295 - acc: 0.5557\n",
      "Epoch 100/100\n",
      "435/435 [==============================] - 9s 21ms/step - loss: 2.4708 - acc: 0.5568\n",
      "--------------------------------------------------------------------------\n",
      "Round: 0 Batch: 9\n",
      "--------------------------------------------------------------------------\n",
      "Getting data: 9000 1000\n",
      "(442, 20, 10000)\n",
      "a further UNK is that the army garrison has been ordered to move on from liberty we will no longer\n",
      "Epoch 1/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 4.2396 - acc: 0.5140\n",
      "Epoch 2/100\n",
      "442/442 [==============================] - 10s 22ms/step - loss: 3.8397 - acc: 0.5650\n",
      "Epoch 3/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.7819 - acc: 0.5655\n",
      "Epoch 4/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.7310 - acc: 0.5655\n",
      "Epoch 5/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.7216 - acc: 0.5666\n",
      "Epoch 6/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.6694 - acc: 0.5663\n",
      "Epoch 7/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.6509 - acc: 0.5674\n",
      "Epoch 8/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.6218 - acc: 0.5691\n",
      "Epoch 9/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.6128 - acc: 0.5693\n",
      "Epoch 10/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.5878 - acc: 0.5699\n",
      "Epoch 11/100\n",
      "442/442 [==============================] - 9s 21ms/step - loss: 3.5811 - acc: 0.5688\n",
      "Epoch 12/100\n",
      "400/442 [==========================>...] - ETA: 0s - loss: 3.5186 - acc: 0.5721"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "start_point = 8000\n",
    "batch = 8\n",
    "round = 0\n",
    "while True:\n",
    "    print(\"Getting data:\", start_point, batch_size)\n",
    "    x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_dict)\n",
    "    y = np.argmax(y_vec, axis=2)\n",
    "    print(\" \".join([idx_dict[x] for x in y[0] ]) )\n",
    "    \n",
    "    if len(x_vec) == 0:\n",
    "        start_point = 0\n",
    "        batch = 0\n",
    "        x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_dict)\n",
    "        round += 1 \n",
    "    model.fit(x_vec, y_vec, batch_size=100, nb_epoch=100, shuffle = True)\n",
    "    model.save_weights(str(batch%2)+\"new_model.h5\")\n",
    "    start_point += batch_size\n",
    "    batch += 1\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Round:\", round, \"Batch:\", batch)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do you clean\n",
    "sen = \"fine fine fine fine fine fine fine fine fine fine fine\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '').split(\" \")\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "#print(res)\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0] ])\n",
    "#if idx_dict[x]!= 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

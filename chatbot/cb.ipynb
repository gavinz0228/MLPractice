{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path, separator, name_idx, content_idx, limit = 10000):\n",
    "    ln = 1\n",
    "    prev_name = None\n",
    "    sentences = []\n",
    "    all_words = set()\n",
    "    for line in open(file_path, 'r', encoding=\"utf-8\"):\n",
    "        #print(ln)\n",
    "        if ln >= limit:\n",
    "            break\n",
    "        ln += 1\n",
    "        items = line.split(separator)\n",
    "        name = items[name_idx].lower()\n",
    "        content = items[content_idx].lower().replace('?', '').replace('!', '').replace( '.', '')\n",
    "        words = content.split()\n",
    "        all_words.update(words)\n",
    "        if prev_name != name:\n",
    "            sentences.append(words)\n",
    "        else:\n",
    "            #print(prev_name, \"spoke again\")\n",
    "            sentences[-1].extend([\" \"] + words )\n",
    "        prev_name = name\n",
    "    return sentences, list(all_words)\n",
    "\n",
    "\n",
    "\n",
    "sentences, all_words = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_dict(all_words, padding_char, unknown_char):\n",
    "    all_words.insert(0, padding_char)\n",
    "    all_words.append(unknown_char)\n",
    "    vocab_dict = {word: id for id, word in enumerate(all_words)}\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = build_vocab_dict(all_words, ' ', 'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dict = {idx:word for word, idx in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"vocab_dict.json\", 'w') \n",
    "f.write(json.dumps(idx_dict)) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 0]\n",
    "y_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentences, vocab_dict, unknown_char, sentence_length):\n",
    "    l = len(sentences)\n",
    "    vec = []\n",
    "    unk_idx = vocab_dict[unknown_char]\n",
    "\n",
    "    for sen in sentences:\n",
    "        vec.append( [vocab_dict[x] if x in vocab_dict else unk_idx for x in sen ][:20])\n",
    "\n",
    "    padded = pad_sequences(vec, maxlen=sentence_length, dtype='int32')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(vec, sentence_length, vocab_length):\n",
    "    res = np.zeros((len(vec), sentence_length, vocab_length))\n",
    "    for i, sen in enumerate(vec):\n",
    "        for j, num in enumerate(sen):\n",
    "            res[i, j, num] = 1\n",
    "    return res\n",
    "\n",
    "x_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 0]\n",
    "y_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 1]\n",
    "\n",
    "x_vec = sentence_to_vec(x_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "y_vec = sentence_to_vec(y_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "y_vec = to_one_hot(y_vec, SENTENCE_LENGTH, len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 20, 1024)          2041856   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "repeat_vector_8 (RepeatVecto (None, 20, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 20, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 20, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 20, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 20, 1994)          2043850   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20, 1994)          0         \n",
      "=================================================================\n",
      "Total params: 37,656,522\n",
      "Trainable params: 37,656,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(x_vocab_len, x_max_len, y_vocab_len, y_max_len, hidden_size, num_layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Creating encoder network\n",
    "    model.add(Embedding(x_vocab_len, 1024, input_length=x_max_len, mask_zero=True))\n",
    "    model.add(LSTM(hidden_size))\n",
    "    model.add(RepeatVector(y_max_len))\n",
    "\n",
    "    # Creating decoder network\n",
    "    for _ in range(num_layers):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(y_vocab_len)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n",
    "model = create_model(len(vocab_dict), SENTENCE_LENGTH, len(vocab_dict), SENTENCE_LENGTH, 1024, 3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.1312 - acc: 0.5891\n",
      "Epoch 2/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.1479 - acc: 0.5872\n",
      "Epoch 3/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.1535 - acc: 0.5839\n",
      "Epoch 4/20\n",
      "434/434 [==============================] - 32s 73ms/step - loss: 2.0303 - acc: 0.5881\n",
      "Epoch 5/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.1266 - acc: 0.5889\n",
      "Epoch 6/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 1.9926 - acc: 0.5944\n",
      "Epoch 7/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.1680 - acc: 0.5850\n",
      "Epoch 8/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.9881 - acc: 0.5889\n",
      "Epoch 9/20\n",
      "434/434 [==============================] - 31s 73ms/step - loss: 2.0199 - acc: 0.5910\n",
      "Epoch 10/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.0704 - acc: 0.5891\n",
      "Epoch 11/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 1.9834 - acc: 0.5900\n",
      "Epoch 12/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 1.9358 - acc: 0.5914\n",
      "Epoch 13/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 1.9560 - acc: 0.5937\n",
      "Epoch 14/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 2.0485 - acc: 0.5856\n",
      "Epoch 15/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.9617 - acc: 0.5945\n",
      "Epoch 16/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.9890 - acc: 0.5919\n",
      "Epoch 17/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.9374 - acc: 0.5925\n",
      "Epoch 18/20\n",
      "434/434 [==============================] - 31s 72ms/step - loss: 1.9392 - acc: 0.5939\n",
      "Epoch 19/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.8772 - acc: 0.5954\n",
      "Epoch 20/20\n",
      "434/434 [==============================] - 31s 71ms/step - loss: 1.9214 - acc: 0.5941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x30415f98>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_vec, y_vec, batch_size=100, nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"chatbot_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                  to to me'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"Let me see what I can do\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '')\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "\n",
    "#print(list(res[19]))\n",
    "#print(res.shape)\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": [{\"class_name\": \"Embedding\", \"config\": {\"name\": \"embedding_9\", \"trainable\": true, \"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"input_dim\": 1994, \"output_dim\": 1024, \"embeddings_initializer\": {\"class_name\": \"RandomUniform\", \"config\": {\"minval\": -0.05, \"maxval\": 0.05, \"seed\": null}}, \"embeddings_regularizer\": null, \"activity_regularizer\": null, \"embeddings_constraint\": null, \"mask_zero\": true, \"input_length\": 20}}, {\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_30\", \"trainable\": true, \"return_sequences\": false, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"units\": 1024, \"activation\": \"tanh\", \"recurrent_activation\": \"hard_sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 1}}, {\"class_name\": \"RepeatVector\", \"config\": {\"name\": \"repeat_vector_8\", \"trainable\": true, \"n\": 20}}, {\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_31\", \"trainable\": true, \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"units\": 1024, \"activation\": \"tanh\", \"recurrent_activation\": \"hard_sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 1}}, {\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_32\", \"trainable\": true, \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"units\": 1024, \"activation\": \"tanh\", \"recurrent_activation\": \"hard_sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 1}}, {\"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_33\", \"trainable\": true, \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"units\": 1024, \"activation\": \"tanh\", \"recurrent_activation\": \"hard_sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"recurrent_initializer\": {\"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 1}}, {\"class_name\": \"TimeDistributed\", \"config\": {\"name\": \"time_distributed_8\", \"trainable\": true, \"layer\": {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_8\", \"trainable\": true, \"units\": 1994, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}}}, {\"class_name\": \"Activation\", \"config\": {\"name\": \"activation_8\", \"trainable\": true, \"activation\": \"softmax\"}}], \"keras_version\": \"2.1.5\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

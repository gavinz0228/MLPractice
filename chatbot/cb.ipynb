{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding,Dropout, Bidirectional, Input, merge, Flatten, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras import backend\n",
    "import keras\n",
    "import json\n",
    "from data_utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 20\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "def load_cornell_movie_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, start_line, limit)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_twitter_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_sentences(\"ShortenTwitterAsciiCorpus.txt\")[start_line:start_line+limit]\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_friends_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"friends-final.txt\", \"\t\", 2, 5, start_line = 0, limit = 10000)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "def load_fake_data(start_line, limit, vocab_dict):\n",
    "    sentences = [[\"hi\"], [\"how\",\"is\", \"it\", \"going\" ], [\"i\",\"am\", \"good\"], [\"that\",\"is\", \"great\", \"to\",\"hear\"],[\"hello\"],[\"how\",\"is\", \"it\", \"going\" ], [\"hey\"], [\"hey\", \"there\"]  ]\n",
    "    for i in range(4):\n",
    "        sentences += sentences\n",
    "    #sentences = [ [\"i'm\", \"not\", \"bad\"], [\"that's\", \"great\", \"to\",\"hear\"]]\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/blob/master/train_bot.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "vocab_to_idx, idx_to_vocab, embeding_matrix = get_vocab('glove.6B.300d.txt', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embeding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gavin\\Projects\\ML\\MLPractice\\chatbot\\model.py:37: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  output_att = merge([inputs, att_prob ], mode='mul')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "#vocab_dict, idx_dict = load_vocab(\"vocab_dict.json\")\n",
    "#model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 100, 3)\n",
    "model = create_model_with_attention(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, embeding_matrix, 300, 100, 3)\n",
    "#print(model.summary())\n",
    "model.load_weights(\"0new_model.h5\")\n",
    "\n",
    "opt = RMSprop(lr=0.0001,rho=0.9, epsilon=None, decay=1e-7)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(backend.get_value(opt.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data: 5000 4000\n",
      "(1763, 20, 30000)\n",
      "                                do you see ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/20\n",
      "1763/1763 [==============================] - 24s 14ms/step - loss: 7.1544 - acc: 0.3414\n",
      "Epoch 2/20\n",
      "1763/1763 [==============================] - 22s 12ms/step - loss: 6.4967 - acc: 0.3605\n",
      "Epoch 3/20\n",
      "1763/1763 [==============================] - 20s 11ms/step - loss: 6.0713 - acc: 0.3732\n",
      "Epoch 4/20\n",
      "1763/1763 [==============================] - 21s 12ms/step - loss: 5.7272 - acc: 0.3847\n",
      "Epoch 5/20\n",
      "1763/1763 [==============================] - 23s 13ms/step - loss: 5.4491 - acc: 0.3941\n",
      "Epoch 6/20\n",
      "1763/1763 [==============================] - 22s 13ms/step - loss: 5.2120 - acc: 0.4068\n",
      "Epoch 7/20\n",
      "1763/1763 [==============================] - 23s 13ms/step - loss: 5.0095 - acc: 0.4189\n",
      "Epoch 8/20\n",
      "1763/1763 [==============================] - 23s 13ms/step - loss: 4.8364 - acc: 0.4265\n",
      "Epoch 9/20\n",
      "1763/1763 [==============================] - 20s 11ms/step - loss: 4.6816 - acc: 0.4381\n",
      "Epoch 10/20\n",
      " 150/1763 [=>............................] - ETA: 20s - loss: 4.3131 - acc: 0.4517"
     ]
    }
   ],
   "source": [
    "batch_size = 4000\n",
    "start_point = 5000\n",
    "#21000\n",
    "round = 0\n",
    "\n",
    "#x_vec, y_vec = load_friends_data(start_point, batch_size, vocab_to_idx) \n",
    "while True: \n",
    "    print(\"Getting data:\", start_point, batch_size) \n",
    "    x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_to_idx)\n",
    "\n",
    "    if len(x_vec) == 0:\n",
    "        start_point = 0\n",
    "        x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_to_idx)\n",
    "        round += 1 \n",
    "    y = np.argmax(y_vec, axis=2)\n",
    "    print(\" \".join([idx_to_vocab[x] for x in y[0] ]) )\n",
    "    last_acc = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        history = model.fit(x_vec, y_vec, batch_size=50, nb_epoch=20, shuffle = True)\n",
    "        model.save_weights(str(i%2)+\"new_model.h5\")\n",
    "        \n",
    "        if abs( history.history[\"acc\"][0] - last_acc ) < 0.02:\n",
    "            if backend.get_value(opt.lr) == 0.0001:\n",
    "                backend.set_value(opt.lr, 0.00001)\n",
    "            else:\n",
    "                backend.set_value(opt.lr, 0.0001)\n",
    "        else:\n",
    "            backend.set_value(opt.lr, 0.001)\n",
    "            \n",
    "        if history.history[\"acc\"][0] > 0.95:\n",
    "            backend.set_value(opt.lr, 0.00001)\n",
    "        print(\"set learning rate: \" , backend.get_value(opt.lr) )\n",
    "        #if  abs( history.history[\"acc\"][0] - last_acc ) >= 0.009 and history.history[\"acc\"][0] < 0.985:\n",
    "        if history.history[\"acc\"][0] < 0.99:\n",
    "            print(last_acc, history.history[\"acc\"][0], \" continue for the same dataset until requirement is satisfied.\")\n",
    "        else:\n",
    "            break\n",
    "        last_acc = history.history[\"acc\"][0]\n",
    "        \n",
    "        i += 1\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(\"Round:\", round,\"Data Start Point\", start_point)\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "    start_point += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do you clean\n",
    "sen = \"are you stupid ?\"\n",
    "sen =  sen .lower().split(\" \")\n",
    "print(sen)\n",
    "vec = sentence_to_vec([sen], vocab_to_idx, 'UNK', SENTENCE_LENGTH)\n",
    "\n",
    "res = model.predict(vec)\n",
    "#print(res)\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_to_vocab[x] for x in vec_y[0] ])\n",
    "#if idx_dict[x]!= 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_idx[\"hey\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path, separator, name_idx, content_idx, start_line = 0, limit = 10000):\n",
    "    ln = 1\n",
    "    line_added = 0\n",
    "    prev_name = None\n",
    "    sentences = []\n",
    "    for line in open(file_path, 'r', encoding=\"utf-8\"):\n",
    "\n",
    "        if ln >= start_line + limit:\n",
    "            if line_added % 2 == 0:\n",
    "                break\n",
    "            else:\n",
    "                limit += 1\n",
    "        elif ln < start_line:\n",
    "            ln += 1\n",
    "            continue\n",
    "        ln += 1\n",
    "        items = line.split(separator)\n",
    "        name = items[name_idx].lower()\n",
    "        content = items[content_idx].lower().replace('?', '').replace('!', '').replace( '.', '').replace( ',', '').replace( '-', '')\n",
    "        words = content.split()\n",
    "        if prev_name != name:\n",
    "            sentences.append(words)\n",
    "            line_added +=1\n",
    "        else:\n",
    "            #print(prev_name, \"spoke again\")\n",
    "            sentences[-1].extend(words )\n",
    "        prev_name = name\n",
    "    #print(len(sentences))\n",
    "    if len(sentences) % 2 != 0:\n",
    "        sentences.pop();\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten_sentences = [word for sentence in sentences for word in sentence ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab_dict(sentences, padding_char, unknown_char):\n",
    "    word_freq = nltk.FreqDist(np.hstack(sentences))\n",
    "    word_freq.pop(padding_char, None)\n",
    "    word_freq.pop(unknown_char, None)\n",
    "    vocab = word_freq.most_common(VOCAB_SIZE - 2)\n",
    "\n",
    "    vocab.insert(0, (padding_char,1))\n",
    "    vocab.append( (unknown_char,1))\n",
    "    \n",
    "    vocab_dict = {pair[0]: id for id, pair in enumerate(vocab)}\n",
    "    \n",
    "    idx_dict = {idx:word for word, idx in vocab_dict.items()}\n",
    "    print(idx_dict)\n",
    "    return vocab_dict, idx_dict\n",
    "\n",
    "\n",
    "#print(vocab_dict, idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab_dict):\n",
    "    f = open(\"vocab_dict.json\", 'w') \n",
    "    f.write(json.dumps(vocab_dict)) \n",
    "    f.close() \n",
    "def load_vocab():\n",
    "    f = open(\"vocab_dict.json\", 'r') \n",
    "    vocab_dict = json.loads(f.read() )\n",
    "    f.close() \n",
    "    idx_dict = idx_dict = {idx:word for word, idx in vocab_dict.items()}\n",
    "    return vocab_dict, idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentences, vocab_dict, unknown_char, sentence_length):\n",
    "    l = len(sentences)\n",
    "    vec = []\n",
    "    unk_idx = vocab_dict[unknown_char]\n",
    "\n",
    "    for sen in sentences:\n",
    "        vec.append( [vocab_dict[x] if x in vocab_dict else unk_idx for x in sen ][:20])\n",
    "\n",
    "    padded = pad_sequences(vec, maxlen=sentence_length, dtype='int32')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(vec, sentence_length, vocab_length):\n",
    "    print((len(vec), sentence_length, vocab_length))\n",
    "    res = np.zeros((len(vec), sentence_length, vocab_length))\n",
    "    for i, sen in enumerate(vec):\n",
    "        for j, num in enumerate(sen):\n",
    "            res[i, j, num] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_vocab_len, x_max_len, y_vocab_len, y_max_len, hidden_size, num_layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Creating encoder network\n",
    "    model.add(Embedding(x_vocab_len, 1024, input_length=x_max_len, mask_zero=True))\n",
    "    model.add(LSTM(hidden_size))\n",
    "    model.add(RepeatVector(y_max_len))\n",
    "\n",
    "    # Creating decoder network\n",
    "    for _ in range(num_layers):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(y_vocab_len)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, 0, 1000)\\n\\nvocab_dict, idx_dict = build_vocab_dict(sentences, \\' \\', \\'UNK\\')\\n#save_vocab(vocab_dict)\\nvocab_dict, idx_dict = load_vocab()\\n#print(vocab_dict, idx_dict)\\n\\n#sentences=[\"hi\",\"i\\'m good\",\"hello\",\"hello\",\"hey\",\"hey there\", \\'shit\\',\\'i know right\\']\\n#sentences= sentences + sentences + sentences + sentences\\n#sentences= sentences + sentences + sentences + sentences\\n#sentences= sentences + sentences + sentences + sentences\\n\\n\\nx_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 0]\\ny_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 1]\\n\\nx_vec = sentence_to_vec(x_sentences, vocab_dict, \\'UNK\\', SENTENCE_LENGTH)\\ny_vec = sentence_to_vec(y_sentences, vocab_dict, \\'UNK\\', SENTENCE_LENGTH)\\ny_vec = to_one_hot(y_vec, SENTENCE_LENGTH, VOCAB_SIZE)\\n\\nprint(x_vec.shape, y_vec.shape)\\n\\nmodel = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 1024, 3)\\n#model.summary()\\nmodel.load_weights(\"new_chatbot_model.h5\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCE_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\"\"\"\n",
    "sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, 0, 1000)\n",
    "\n",
    "vocab_dict, idx_dict = build_vocab_dict(sentences, ' ', 'UNK')\n",
    "#save_vocab(vocab_dict)\n",
    "vocab_dict, idx_dict = load_vocab()\n",
    "#print(vocab_dict, idx_dict)\n",
    "\n",
    "#sentences=[\"hi\",\"i'm good\",\"hello\",\"hello\",\"hey\",\"hey there\", 'shit','i know right']\n",
    "#sentences= sentences + sentences + sentences + sentences\n",
    "#sentences= sentences + sentences + sentences + sentences\n",
    "#sentences= sentences + sentences + sentences + sentences\n",
    "\n",
    "\n",
    "x_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 0]\n",
    "y_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 1]\n",
    "\n",
    "x_vec = sentence_to_vec(x_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "y_vec = sentence_to_vec(y_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "y_vec = to_one_hot(y_vec, SENTENCE_LENGTH, VOCAB_SIZE)\n",
    "\n",
    "print(x_vec.shape, y_vec.shape)\n",
    "\n",
    "model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 1024, 3)\n",
    "#model.summary()\n",
    "model.load_weights(\"new_chatbot_model.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, start_line, limit)\n",
    "\n",
    "    x_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 0]\n",
    "    y_sentences = [sentences[i] for i in range(len(sentences)) if i % 2 == 1]\n",
    "\n",
    "    x_vec = sentence_to_vec(x_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "    y_vec = sentence_to_vec(y_sentences, vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "    y_vec = to_one_hot(y_vec, SENTENCE_LENGTH, VOCAB_SIZE)\n",
    "\n",
    "    print(x_vec.shape, y_vec.shape)\n",
    "    return x_vec, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 1024, 3)\n",
    "#model.summary()\n",
    "model.load_weights(\"new_chatbot_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict, idx_dict = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data: 216000 1000\n",
      "(474, 20, 10000)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-2ad4c8ad354f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Getting data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b22286e07ee3>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(start_line, limit, vocab_dict)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSENTENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2b07df251274>\u001b[0m in \u001b[0;36mto_one_hot\u001b[0;34m(vec, sentence_length, vocab_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "start_point = 216000\n",
    "batch = 216\n",
    "round = 0\n",
    "while True:\n",
    "    print(\"Getting data:\", start_point, batch_size)\n",
    "    x_vec, y_vec = load_data(start_point, batch_size, vocab_dict)\n",
    "    if len(x_vec) == 0:\n",
    "        start_point = 0\n",
    "        batch = 0\n",
    "        x_vec, y_vec = load_data(start_point, batch_size, vocab_dict)\n",
    "        round += 1\n",
    "    model.fit(x_vec, y_vec, batch_size=200, nb_epoch=1000, verbose =3)\n",
    "    model.save_weights(\"new_chatbot_model.h5\")\n",
    "    start_point += batch_size\n",
    "    batch += 1\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Round:\", round, \"Batch:\", batch)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    part             don't           if a you him you\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"died\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '')\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                  be     so just and up that if that's please\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = \"what do you mean\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '')\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0] if idx_dict[x]!= 'UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding,Dropout, Bidirectional, Input, merge\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import json\n",
    "from data_utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "def load_cornell_movie_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, start_line, limit)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_twitter_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_sentences(\"ShortenTwitterAsciiCorpus.txt\")[1000:2000]\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\Project\\MLPractice\\chatbot\\model.py:29: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  output_att = merge([inputs, att_prob ], mode='mul')\n",
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 200)      240600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 200)      40200       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 20, 200)      0           bidirectional_1[0][0]            \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 20, 200)      240600      merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 20, 200)      240600      gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, 20, 200)      240600      gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 10000)    2010000     gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 10000)    0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 6,012,600\n",
      "Trainable params: 6,012,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 100, 3)\n",
    "model = create_model_with_attention(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 300, 100, 3)\n",
    "print(model.summary())\n",
    "#model.load_weights(\"0new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict, idx_dict = load_vocab(\"vocab_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data: 0 1000\n",
      "(434, 20, 10000)\n",
      "(434, 20)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434/434 [==============================] - 10s 23ms/step - loss: 4.5356 - acc: 0.4019\n",
      "Epoch 2/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 4.2620 - acc: 0.4156\n",
      "Epoch 3/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 4.0922 - acc: 0.4160\n",
      "Epoch 4/100\n",
      "434/434 [==============================] - 10s 23ms/step - loss: 3.9955 - acc: 0.4173\n",
      "Epoch 5/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.8938 - acc: 0.4207\n",
      "Epoch 6/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.7600 - acc: 0.4237\n",
      "Epoch 7/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.6932 - acc: 0.4262\n",
      "Epoch 8/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.6199 - acc: 0.4274\n",
      "Epoch 9/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.5861 - acc: 0.4300\n",
      "Epoch 10/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.5154 - acc: 0.4270\n",
      "Epoch 11/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.4870 - acc: 0.4281\n",
      "Epoch 12/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.4341 - acc: 0.4309\n",
      "Epoch 13/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.4541 - acc: 0.4305\n",
      "Epoch 14/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.3785 - acc: 0.4327\n",
      "Epoch 15/100\n",
      "434/434 [==============================] - 10s 22ms/step - loss: 3.3265 - acc: 0.4361\n",
      "Epoch 16/100\n",
      "434/434 [==============================] - 9s 22ms/step - loss: 3.3211 - acc: 0.4348\n",
      "Epoch 17/100\n",
      "434/434 [==============================] - 9s 22ms/step - loss: 3.3116 - acc: 0.4338\n",
      "Epoch 18/100\n",
      "200/434 [============>.................] - ETA: 5s - loss: 3.3087 - acc: 0.4237"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "start_point = 000\n",
    "batch = 0\n",
    "round = 0\n",
    "while True:\n",
    "    print(\"Getting data:\", start_point, batch_size)\n",
    "    x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_dict)\n",
    "    print(x_vec.shape)\n",
    "    if len(x_vec) == 0:\n",
    "        start_point = 0\n",
    "        batch = 0\n",
    "        x_vec, y_vec = load_cornell_movie_data(start_point, batch_size, vocab_dict)\n",
    "        round += 1 \n",
    "    model.fit(x_vec, y_vec, batch_size=100, nb_epoch=100, shuffle = True)\n",
    "    model.save_weights(str(batch%2)+\"new_model.h5\")\n",
    "    start_point += batch_size\n",
    "    batch += 1\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Round:\", round, \"Batch:\", batch)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do you clean\n",
    "sen = \"what do you like to do\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '').split(\" \")\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0] if idx_dict[x]!= 'UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

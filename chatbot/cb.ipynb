{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding,Dropout, Bidirectional, Input, merge, Flatten, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import json\n",
    "from data_utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "def load_cornell_movie_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"movie_lines.txt\", \" +++$+++ \", 3, 4, start_line, limit)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_twitter_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_sentences(\"ShortenTwitterAsciiCorpus.txt\")[start_line:start_line+limit]\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec\n",
    "\n",
    "def load_friends_data(start_line, limit, vocab_dict):\n",
    "    sentences = read_lines(\"friends-final.txt\", \"\t\", 2, 5, start_line = 0, limit = 10000)\n",
    "    x_vec, y_vec = vectorize_sentence(sentences, vocab_dict, VOCAB_SIZE, SENTENCE_LENGTH)\n",
    "    return x_vec, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gavin\\Projects\\ML\\MLPractice\\chatbot\\model.py:36: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  output_att = merge([inputs, att_prob ], mode='mul')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 300)      3000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 20, 200)      320800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4000)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 20, 4000)     0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20, 200)      800200      repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 20, 200)      0           bidirectional_2[0][0]            \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 20, 200)      320800      merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 20, 200)      320800      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 20, 200)      320800      lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 10000)    2010000     lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 10000)    0           time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 7,093,400\n",
      "Trainable params: 7,093,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model = create_model(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 100, 3)\n",
    "model = create_model_with_attention(VOCAB_SIZE, SENTENCE_LENGTH, VOCAB_SIZE, SENTENCE_LENGTH, 300, 100, 3)\n",
    "print(model.summary())\n",
    "model.load_weights(\"0new_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict, idx_dict = load_vocab(\"vocab_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data: 0 1000\n",
      "(4885, 20, 10000)\n",
      "                  there's nothing to tell he's just some guy i work with\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4885/4885 [==============================] - 115s 24ms/step - loss: 3.9766 - acc: 0.5478\n",
      "Epoch 2/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 3.5227 - acc: 0.5646\n",
      "Epoch 3/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 3.3787 - acc: 0.5679\n",
      "Epoch 4/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 3.2779 - acc: 0.5695\n",
      "Epoch 5/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 3.1977 - acc: 0.5709\n",
      "Epoch 6/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 3.1317 - acc: 0.5720\n",
      "Epoch 7/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 3.0784 - acc: 0.5728\n",
      "Epoch 8/100\n",
      "4885/4885 [==============================] - 125s 26ms/step - loss: 3.0149 - acc: 0.5746\n",
      "Epoch 9/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.9777 - acc: 0.5749\n",
      "Epoch 10/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.9152 - acc: 0.5769\n",
      "Epoch 11/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.8843 - acc: 0.5776\n",
      "Epoch 12/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.8340 - acc: 0.5794\n",
      "Epoch 13/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.7903 - acc: 0.5806\n",
      "Epoch 14/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.7529 - acc: 0.5824\n",
      "Epoch 15/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.7206 - acc: 0.5841\n",
      "Epoch 16/100\n",
      "4885/4885 [==============================] - 116s 24ms/step - loss: 2.6852 - acc: 0.5863\n",
      "Epoch 17/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.6536 - acc: 0.5876\n",
      "Epoch 18/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.6120 - acc: 0.5908\n",
      "Epoch 19/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.5914 - acc: 0.5914\n",
      "Epoch 20/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.5570 - acc: 0.5940\n",
      "Epoch 21/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.5456 - acc: 0.5961\n",
      "Epoch 22/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.5106 - acc: 0.5983\n",
      "Epoch 23/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.4893 - acc: 0.6002\n",
      "Epoch 24/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.4600 - acc: 0.6025\n",
      "Epoch 25/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.4366 - acc: 0.6042\n",
      "Epoch 26/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.4126 - acc: 0.6066\n",
      "Epoch 27/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.3933 - acc: 0.6088\n",
      "Epoch 28/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.3695 - acc: 0.6112\n",
      "Epoch 29/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.3424 - acc: 0.6132\n",
      "Epoch 30/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.3187 - acc: 0.6154\n",
      "Epoch 31/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.2952 - acc: 0.6175\n",
      "Epoch 32/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.2694 - acc: 0.6195\n",
      "Epoch 33/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.2524 - acc: 0.6208\n",
      "Epoch 34/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.2309 - acc: 0.6244\n",
      "Epoch 35/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.1981 - acc: 0.6266\n",
      "Epoch 36/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.1844 - acc: 0.6290\n",
      "Epoch 37/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.1585 - acc: 0.6320\n",
      "Epoch 38/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 2.1346 - acc: 0.6341\n",
      "Epoch 39/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.1146 - acc: 0.6364\n",
      "Epoch 40/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.1006 - acc: 0.6377\n",
      "Epoch 41/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 2.0738 - acc: 0.6412\n",
      "Epoch 42/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.0497 - acc: 0.6443\n",
      "Epoch 43/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.0292 - acc: 0.6463\n",
      "Epoch 44/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 2.0141 - acc: 0.6480\n",
      "Epoch 45/100\n",
      "4885/4885 [==============================] - 114s 23ms/step - loss: 1.9890 - acc: 0.6506\n",
      "Epoch 46/100\n",
      "4885/4885 [==============================] - 115s 23ms/step - loss: 1.9714 - acc: 0.6534\n",
      "Epoch 47/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 1.9491 - acc: 0.6563\n",
      "Epoch 48/100\n",
      "4885/4885 [==============================] - 115s 24ms/step - loss: 1.9254 - acc: 0.6583\n",
      "Epoch 49/100\n",
      "4100/4885 [========================>.....] - ETA: 19s - loss: 1.8979 - acc: 0.6630"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "start_point = 0000\n",
    "batch = 0\n",
    "round = 0\n",
    "while True:\n",
    "    print(\"Getting data:\", start_point, batch_size)\n",
    "    x_vec, y_vec = load_friends_data(start_point, batch_size, vocab_dict)\n",
    "    \n",
    "    \n",
    "    if len(x_vec) == 0:\n",
    "        start_point = 0\n",
    "        batch = 0\n",
    "        x_vec, y_vec = load_friends_data(start_point, batch_size, vocab_dict)\n",
    "        round += 1 \n",
    "    y = np.argmax(y_vec, axis=2)\n",
    "    print(\" \".join([idx_dict[x] for x in y[0] ]) )\n",
    "    model.fit(x_vec, y_vec, batch_size=100, nb_epoch=100, shuffle = True)\n",
    "    model.save_weights(str(batch%2)+\"new_model.h5\")\n",
    "    start_point += batch_size\n",
    "    batch += 1\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "    print(\"Round:\", round, \"Batch:\", batch)\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                  have i here'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do you clean\n",
    "sen = \"what's your name\"\n",
    "sen = sen.lower().replace('?', '').replace('!', '').replace( '.', '').split(\" \")\n",
    "vec = sentence_to_vec([sen], vocab_dict, 'UNK', SENTENCE_LENGTH)\n",
    "#print(model.predict(vec).shape)\n",
    "res = model.predict(vec)\n",
    "#print(res)\n",
    "vec_y = np.argmax(res, axis=2)\n",
    "\" \".join([idx_dict[x] for x in vec_y[0] ])\n",
    "#if idx_dict[x]!= 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
